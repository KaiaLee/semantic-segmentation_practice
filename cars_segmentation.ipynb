{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaiaLee/semantic-segmentation_practice/blob/main/cars_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PaxoqFkMAVhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd9ed52-df14-4a62-e35c-0a22c7697849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/Cars\n"
          ]
        }
      ],
      "source": [
        "# Mount to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Move to your current working directory\n",
        "%cd drive/MyDrive/Colab\\ Notebooks/Cars"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the packages\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s3gL3LFHAed8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build one of the main components - DoubleConv - for UNet\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ],
      "metadata": {
        "id": "OQospNIGB9CM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build UNet from scrach\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "    super().__init__()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.pool = nn.MaxPool2d(2, 2) ##\n",
        "    # Down part of UNet\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "    # Up part of UNet\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(nn.ConvTranspose2d(feature*2, feature, 2, 2))\n",
        "      self.ups.append(DoubleConv(feature*2, feature))\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, 1)\n",
        "  def forward(self, x):\n",
        "    skip_connections = []\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "    x = self.bottleneck(x)\n",
        "    skip_connections.reverse() #直接在原始列表上進行反轉，而不會返回一個新的列表\n",
        "    #skip_connections = skip_connections[::-1] #從列表的最後一個元素到第一個元素，步長為 -1，反轉列表\n",
        "    for i in range(0, len(self.ups), 2): #生成一個從 0 到 self.ups 長度的範圍，每次增量為 2。這樣的循環可以用於遍歷上升（up）部分的每一對層。\n",
        "      x = self.ups[i](x) #對於每個偶數索引 i，這行將 x 輸入到對應的上升層\n",
        "      skip_connection = skip_connections[i//2] #這行從反轉的 skip_connections 列表中選取相應的跳接特徵圖，索引為 i//2，因為每次迭代中 i 增加 2(ConvTranspose+DoubleConv這兩個為一組)\n",
        "      concat_skip = torch.cat((skip_connection, x), dim=1) # N xC x H x W\n",
        "      x = self.ups[i+1](concat_skip)\n",
        "    return self.final_conv(x)"
      ],
      "metadata": {
        "id": "rZSFGD-sCHtV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an UNet model object\n",
        "model = UNet()\n",
        "toy_data = torch.rand((16, 3, 160, 240))\n",
        "output = model(toy_data)\n",
        "print(output.shape)\n",
        "# Move the model to GPU\n",
        "#model = model.cuda()"
      ],
      "metadata": {
        "id": "MO5Xg0pUJbNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df62a52-8ecd-4330-f114-19f3911e313a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 160, 240])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CustomDataset for loading data from Google Drive\n",
        "class CustomDataset(Dataset):   #使用pytorch已經寫好的(可以丟到dataloader裡、讓data可以用minibanch gradient decent(banch size=16一次loading16張圖，丟給model train))\n",
        "  def __init__(self, image_dir, mask_dir, transform=None):\n",
        "    super().__init__()\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "  def __len__(self):\n",
        "    print(f\"Number of images: {len(self.images)}\")  # 輸出圖像數量\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    print(f\"Fetching item at index: {index}\")  # 輸出當前索引\n",
        "    image_path = os.path.join(self.image_dir, self.images[index]) #資料夾, 檔名\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index].replace('.jpg', '_mask.gif'))\n",
        "    image = np.array(Image.open(image_path)) #pytorch在處理pyarray都會比較快，list轉tensor不會比np.array快，通常都先轉成np.array #一次開一張圖片不爆掉\n",
        "    mask = np.array(Image.open(mask_path).convert('L')) #RGB轉L灰階影像(黑色0和白色255)\n",
        "    return self.transform(image), self.transform(mask) #data pre-processing\n",
        "\n",
        "    if self.transform:  # 確保 transform 不為 None\n",
        "        image = self.transform(image)\n",
        "        mask = self.transform(mask)\n",
        "    return image, mask  # 返回處理後的數據"
      ],
      "metadata": {
        "id": "ILgSgfqFJidq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the device we are using is GPU or CPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "5eqoqZEaHUzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279c8a1f-e267-4745-e0b3-09500c31539c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for UNet model training process\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3\n",
        "IMG_WIDTH = 240\n",
        "IMG_HEIGHT = 160"
      ],
      "metadata": {
        "id": "sBoa09DRHUtm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "all_data = CustomDataset('small_train', 'small_train_masks', T.Compose([T.ToTensor(), T.Resize((IMG_HEIGHT, IMG_WIDTH))]))\n",
        "#pytorch裡的ToTensor會把所有數值除以255，所以不用變0跟1就可丟transform"
      ],
      "metadata": {
        "id": "rd67NulqHUly"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and val\n",
        "train_data, val_data = torch.utils.data.random_split(all_data, [0.7, 0.3])"
      ],
      "metadata": {
        "id": "GwM6Vz5NKtfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994f681a-d07e-4b1d-f982-9164c82b7c52"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 1600\n",
            "Number of images: 1600\n",
            "Number of images: 1600\n",
            "Number of images: 1600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create loader for mini-batch gradient descent\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "Gp2ZXGzHLYmt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The loss function for bianry classification\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Choosing Adam as our optimizer\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "jEJ-RbO6UzJP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, num_epochs, train_loader, optimizer, print_every=30):\n",
        "  for epoch in range(num_epochs):\n",
        "    for count, (x, y) in enumerate(train_loader):\n",
        "      model.train()\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      out = model(x)\n",
        "      if count % print_every == 0:\n",
        "        eval(model, val_loader, epoch)\n",
        "      out = torch.sigmoid(out)\n",
        "      loss = loss_function(out, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ],
      "metadata": {
        "id": "fYS30O-dSl0L"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, val_loader, epoch):\n",
        "  model.eval()\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      out_img = model(x)\n",
        "      probability = torch.sigmoid(out_img)\n",
        "      predictions = probability>0.5\n",
        "      num_correct += (predictions==y).sum()\n",
        "      num_pixels += BATCH_SIZE*IMG_WIDTH*IMG_HEIGHT\n",
        "  print(f'Epoch[{epoch+1}] Acc: {num_correct/num_pixels}')\n"
      ],
      "metadata": {
        "id": "4weW5Wi8RWMW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, NUM_EPOCHS, train_loader, optimizer)"
      ],
      "metadata": {
        "id": "8rcL1usEWTHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5cf62d2-dc57-4e9d-95db-35a3ecf38e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching item at index: 1218\n",
            "Fetching item at index: 449\n",
            "Fetching item at index: 53\n",
            "Fetching item at index: 871\n",
            "Fetching item at index: 875\n",
            "Fetching item at index: 148\n",
            "Fetching item at index: 136\n",
            "Fetching item at index: 69\n",
            "Fetching item at index: 609\n",
            "Fetching item at index: 1088\n",
            "Fetching item at index: 304\n",
            "Fetching item at index: 1486\n",
            "Fetching item at index: 612\n",
            "Fetching item at index: 623\n",
            "Fetching item at index: 1031\n",
            "Fetching item at index: 432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iLmSx5eZGq8t"
      }
    }
  ]
}